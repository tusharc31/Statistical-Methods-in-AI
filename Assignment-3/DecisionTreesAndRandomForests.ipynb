{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0_wToNFHMN3"
   },
   "source": [
    "# **Decision Trees**\n",
    "\n",
    "The Wisconsin Breast Cancer Dataset(WBCD) can be found here(https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
    "\n",
    "This dataset describes the characteristics of the cell nuclei of various patients with and without breast cancer. The task is to classify a decision tree to predict if a patient has a benign or a malignant tumour based on these features.\n",
    "\n",
    "Attribute Information:\n",
    "```\n",
    "#  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qYdlWpUVHMOB"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CT</th>\n",
       "      <th>UCSize</th>\n",
       "      <th>UCShape</th>\n",
       "      <th>MA</th>\n",
       "      <th>SECSize</th>\n",
       "      <th>BN</th>\n",
       "      <th>BC</th>\n",
       "      <th>NN</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.417740</td>\n",
       "      <td>3.134478</td>\n",
       "      <td>3.207439</td>\n",
       "      <td>2.806867</td>\n",
       "      <td>3.216023</td>\n",
       "      <td>3.463519</td>\n",
       "      <td>3.437768</td>\n",
       "      <td>2.866953</td>\n",
       "      <td>1.589413</td>\n",
       "      <td>2.689557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.815741</td>\n",
       "      <td>3.051459</td>\n",
       "      <td>2.971913</td>\n",
       "      <td>2.855379</td>\n",
       "      <td>2.214300</td>\n",
       "      <td>3.640708</td>\n",
       "      <td>2.438364</td>\n",
       "      <td>3.053634</td>\n",
       "      <td>1.715078</td>\n",
       "      <td>0.951273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CT      UCSize     UCShape          MA     SECSize          BN  \\\n",
       "count  699.000000  699.000000  699.000000  699.000000  699.000000  699.000000   \n",
       "mean     4.417740    3.134478    3.207439    2.806867    3.216023    3.463519   \n",
       "std      2.815741    3.051459    2.971913    2.855379    2.214300    3.640708   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "25%      2.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
       "50%      4.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
       "75%      6.000000    5.000000    5.000000    4.000000    4.000000    5.000000   \n",
       "max     10.000000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
       "\n",
       "               BC          NN     Mitoses   Diagnosis  \n",
       "count  699.000000  699.000000  699.000000  699.000000  \n",
       "mean     3.437768    2.866953    1.589413    2.689557  \n",
       "std      2.438364    3.053634    1.715078    0.951273  \n",
       "min      1.000000    1.000000    1.000000    2.000000  \n",
       "25%      2.000000    1.000000    1.000000    2.000000  \n",
       "50%      3.000000    1.000000    1.000000    2.000000  \n",
       "75%      5.000000    4.000000    1.000000    4.000000  \n",
       "max     10.000000   10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "headers = [\"ID\",\"CT\",\"UCSize\",\"UCShape\",\"MA\",\"SECSize\",\"BN\",\"BC\",\"NN\",\"Mitoses\",\"Diagnosis\"]\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', na_values='?',    \n",
    "         header=None, index_col=['ID'], names = headers) \n",
    "data = data.reset_index(drop=True)\n",
    "data = data.fillna(0)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_gQq5qrHMOG"
   },
   "source": [
    "1. a) Implement a decision tree (you can use decision tree implementation from existing libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the accuracy of the classifiers change from run to run and after multiple iterations I have tried report average accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (699, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of data: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "X = data.iloc[:,:9] # features\n",
    "y = data.iloc[:,9]  # class label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, train_size=0.9)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 1/9, train_size = 8/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier() # create the decision tree model object\n",
    "model = model.fit(X_train,y_train) # train the decision tree\n",
    "y_pred = model.predict(X_test) # predict class labels for test data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) # model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ7N9m_mHMOJ"
   },
   "source": [
    "1. b) Train a decision tree object of the above class on the WBC dataset using misclassification rate, entropy and Gini as the splitting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eHFij6PaHMOJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over 100 runs\n",
      "Gini Accuracy: 0.9318571428571419\n",
      "Entropy Accuracy: 0.9377142857142854\n"
     ]
    }
   ],
   "source": [
    "gini_accuracy_sum = 0\n",
    "entropy_accuracy_sum = 0\n",
    "\n",
    "# Average over 100 runs\n",
    "for i in range(100):\n",
    "    \n",
    "    model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "    model = model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    gini_accuracy_sum = gini_accuracy_sum + metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    model = model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    entropy_accuracy_sum = entropy_accuracy_sum + metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "print(\"Average over 100 runs\")\n",
    "print(\"Gini Accuracy:\",gini_accuracy_sum/100)\n",
    "print(\"Entropy Accuracy:\",entropy_accuracy_sum/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXEjInvmHMOK"
   },
   "source": [
    "1. c) Report the accuracies in each of the above splitting metrics and give the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49QZvmgNHMOL"
   },
   "source": [
    "We find that:\n",
    "* Gini Accuracy: 0.9318571428571419\n",
    "* Entropy Accuracy: 0.9377142857142854\n",
    "\n",
    "Both criterion give similar results, with entropy being slightly better in most of the runs.  \n",
    "Avg accuracy is about 93 to 94% and best result is never more than 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz_7nYxPHMON"
   },
   "source": [
    "1. d) Experiment with different approaches to decide when to terminate the tree (number of layers, purity measure, etc). Report and give explanations for all approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after parameter tuning = 0.9428571428571428\n",
      "Parameters for tuned decision tree are:\n",
      "• max_depth = 9\n",
      "• min_impurity_decrease = 0.0\n",
      "• criterion = gini\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################# Hyper-parameters #####################\n",
    "max_depth = [5, 6, 7, 8, 9, 10, 11, 12]\n",
    "min_impurity_decrease = np.linspace(0,1,40)\n",
    "criterion = ['gini', 'entropy']\n",
    "########################################################\n",
    "\n",
    "max_depth_parameter = None\n",
    "min_impurity_decrease_parameter = None\n",
    "criterion_parameter = None\n",
    "\n",
    "highest_acc = -np.inf\n",
    "\n",
    "for mx_depth in max_depth:\n",
    "    for mn_impurity_decrease in min_impurity_decrease:\n",
    "        for criteria in criterion:\n",
    "                    \n",
    "                    accuracy_sum = 0\n",
    "                    \n",
    "                    for i in range(5):\n",
    "                        model = DecisionTreeClassifier(criterion=criteria, max_depth=mx_depth, min_impurity_decrease=mn_impurity_decrease)\n",
    "                        model = model.fit(X_train,y_train)\n",
    "                        y_pred = model.predict(X_val)\n",
    "                        accuracy_sum = accuracy_sum + metrics.accuracy_score(y_val, y_pred)\n",
    "                    \n",
    "                    accuracy_sum = accuracy_sum / 5\n",
    "                    \n",
    "                    if highest_acc < accuracy_sum:\n",
    "                        highest_acc = accuracy_sum\n",
    "                        max_depth_parameter = mx_depth\n",
    "                        min_impurity_decrease_parameter = mn_impurity_decrease\n",
    "                        criterion_parameter = criteria\n",
    "\n",
    "                        \n",
    "model = DecisionTreeClassifier(criterion=criterion_parameter, max_depth=max_depth_parameter, min_impurity_decrease=min_impurity_decrease_parameter)\n",
    "model = model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy after parameter tuning = {acc}\")\n",
    "print(f\"Parameters for tuned decision tree are:\")\n",
    "print(f\"• max_depth = {max_depth_parameter}\")\n",
    "print(f\"• min_impurity_decrease = {min_impurity_decrease_parameter}\")\n",
    "print(f\"• criterion = {criterion_parameter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the decision tree, I have generated multiple possible combinations for 3 of the features. For each set, I have trained 5 decision trees and calculated the average accuracy on validation set. After this I have reported the set of parameters giving highest average accuracy on validation set and reported its accuracy on test set.  \n",
    "\n",
    "The parameters obtained are usually similar to `max_depth = 8`, `min_impurity_decrease = 0.0` and `criterion = entropy`. Maximum depth is usually near 6 to 8. This because as the dataset is not very large, by having more depth we will overfit. Minimum impurity decrease is mostly 0 which tells us about the data. We need to make some cuts with no information gain to be able to classify later. Gini and entropy are both common but gini is slightly less frequent than entropy as it is fast but does less computations than entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWAN_wWXHMOQ"
   },
   "source": [
    "2. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnO5uqHlHMOR"
   },
   "source": [
    "Answer:  \n",
    "\n",
    "* **Bagging** (Bootstrap Aggregating) is used to reduce variance of our prediction by generating training data using the original dataset using combinations with repetitions to produce multiple sets of the same cardinality. Now each of these sets is used to train a model and majority vote is taken to decide the class.\n",
    "\n",
    "* **Boosting**: In this approach we first use subsets of the original data to produce a series of averagely performing models and then boosts their performance by combining them together using a particular cost function. In boosting the subset creation is not random and depends upon the performance of the previous models. Every new subsets contains the elements that were misclassified by previous models.\n",
    "\n",
    "* **Stacking** is a similar to boosting with several different models are used on the data. We estimate the target using outputs of every model.\n",
    "\n",
    "Random forest belongs to bagging as we have multiple trees and our final prediction is based on the prediction of all the trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pihvGbqLHMOS"
   },
   "source": [
    "3. Implement random forest algorithm using different decision trees . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dXdPP2aIHMOT",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TREES = 13    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 24    ACCURACY = 0.9857142857142858\n",
      "NUMBER OF TREES = 31    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 39    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 40    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 45    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 50    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 59    ACCURACY = 0.9857142857142858\n",
      "NUMBER OF TREES = 63    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 78    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 86    ACCURACY = 0.9714285714285714\n",
      "NUMBER OF TREES = 91    ACCURACY = 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def most_common_label(y):\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def RandomForest(n_trees):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        tree = DecisionTreeClassifier(\n",
    "            max_depth = max_depth_parameter,\n",
    "            min_impurity_decrease = min_impurity_decrease_parameter,\n",
    "            criterion = criterion_parameter\n",
    "        )\n",
    "        tree.fit(X_train, y_train)\n",
    "        trees.append(tree)\n",
    "        \n",
    "    tree_preds = np.array([tree.predict(X_val) for tree in trees])\n",
    "    tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "    y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
    "    acc = metrics.accuracy_score(y_pred, y_val)\n",
    "    return acc\n",
    "\n",
    "for i in [13, 24, 31, 39, 40, 45, 50, 59, 63, 78, 86, 91]:\n",
    "    print(f\"NUMBER OF TREES = {i}    ACCURACY = {RandomForest(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJOn5nNZHMOU"
   },
   "source": [
    "4. Report the accuracies obtained after using the Random forest algorithm and compare it with the best accuracies obtained with the decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce4KiiIGHMOV"
   },
   "source": [
    "Answer  \n",
    "In majority of the runs, best perforamce of random forest is better than better performace of single decision tree. Accuracy for both have been printed and can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj-vNvsYHMOX"
   },
   "source": [
    "5. Submit your solution as a separate pdf in the final zip file of your submission\n",
    "\n",
    "\n",
    "Compute a decision tree with the goal to predict the food review based on its smell, taste and portion size.\n",
    "\n",
    "(a) Compute the entropy of each rule in the first stage.\n",
    "\n",
    "(b) Show the final decision tree. Clearly draw it.\n",
    "\n",
    "Submit a handwritten response. Clearly show all the steps.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision_trees.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
